{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"'I am a student from the University of Alabama. I was born in Ontario, Canada and I am a huge fan of the United States. I am going to get a degree in Philosophy to improve my chances of becoming a Philosophy professor. I have been working towards this goal for 4 years. I am currently enrolled in a PhD program. It is very difficult, but I am confident that it will be a good decision'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\"'I am a student from the University of Alabama. I was born in Ontario, Canada and I am a huge fan of the United States. I am going to get a degree in Philosophy to improvemy chances of becoming a Philosophy professor. I have beenworking towards this goal for 4 years. I am currently enrolledin a PhD program. It is very difficult, but I am confident thatit will be a good decision'\""
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You should observe that the computer reads bodies of text, even if punctuated, as single string objects. \n",
    "* Because of this, we need to find a way to separate this single body of text so that the computer evaluates each word as an individual string object. \n",
    "* This brings us to the concept of word tokenization, which is simply the process of separating a single string object, usually a body of text of varying length, into individual tokens that represent words or characters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natural Language Toolkit (NLTK) module .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#NLTK allows you to use some of the more basic NLP functionalities, as well as pretrained models for different tasks.\n",
    "#It is my goal to allow you to train your own models, so we will not be working with any of the pretrained models in NLTK. \n",
    "#However, you should read through the NLTK module documentation to become familiar with certain functions and algorithms that expedite text preprocessing. \n",
    "#Relating back to our example, let’s tokenize the sample data via the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sent_tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[\"'I am a student from the University of Alabama.\", 'I was born in Ontario, Canada and I am a huge fan of the United States.', 'I am going to get a degree in Philosophy to improvemy chances of becoming a Philosophy professor.', 'I have beenworking towards this goal for 4 years.', 'I am currently enrolledin a PhD program.', \"It is very difficult, but I am confident thatit will be a good decision'\"]\n"
    }
   ],
   "source": [
    "sent_tokens = sent_tokenize(sample_text)\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word_tokenize() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[\"'\", 'I', 'am', 'a', 'student', 'from', 'the', 'University', 'of', 'Alabama', '.', 'I', 'was', 'born', 'in', 'Ontario', ',', 'Canada', 'and', 'I', 'am', 'a', 'huge', 'fan', 'of', 'the', 'United', 'States', '.', 'I', 'am', 'going', 'to', 'get', 'a', 'degree', 'in', 'Philosophy', 'to', 'improvemy', 'chances', 'of', 'becoming', 'a', 'Philosophy', 'professor', '.', 'I', 'have', 'beenworking', 'towards', 'this', 'goal', 'for', '4', 'years', '.', 'I', 'am', 'currently', 'enrolledin', 'a', 'PhD', 'program', '.', 'It', 'is', 'very', 'difficult', ',', 'but', 'I', 'am', 'confident', 'thatit', 'will', 'be', 'a', 'good', 'decision', \"'\"]\n"
    }
   ],
   "source": [
    "word_tokens = word_tokenize(sample_text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We split the text sentence/paragraph into a list of words. Each word in the list is called a token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have individual tokens that we can preprocess!\n",
    "\n",
    "From this step forward, we can clean out some of the junk text that we would not want to extract features from. \n",
    "\n",
    "Typically, the first thing we want to get rid of are stop words, which are usually defined as very common words in a given language. Most often, lists of stop words that we build or utilize in software packages include function words , which are words that express a grammatical relationship (rather than having an intrinsic meaning). \n",
    "\n",
    "Examples of function words include the, and, for, and of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import nltk\n",
    "nltk.download('reuters')  # 'punkt', 'stopwords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#Loading stopwords Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the list of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
    }
   ],
   "source": [
    "print (stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['arabic', 'azerbaijani', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n"
    }
   ],
   "source": [
    "print (stopwords.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['छ', 'र', 'पनि', 'छन्', 'लागि', 'भएको', 'गरेको', 'भने', 'गर्न', 'गर्ने', 'हो', 'तथा', 'यो', 'रहेको', 'उनले', 'थियो', 'हुने', 'गरेका', 'थिए', 'गर्दै', 'तर', 'नै', 'को', 'मा', 'हुन्', 'भन्ने', 'हुन', 'गरी', 'त', 'हुन्छ', 'अब', 'के', 'रहेका', 'गरेर', 'छैन', 'दिए', 'भए', 'यस', 'ले', 'गर्नु', 'औं', 'सो', 'त्यो', 'कि', 'जुन', 'यी', 'का', 'गरि', 'ती', 'न', 'छु', 'छौं', 'लाई', 'नि', 'उप', 'अक्सर', 'आदि', 'कसरी', 'क्रमशः', 'चाले', 'अगाडी', 'अझै', 'अनुसार', 'अन्तर्गत', 'अन्य', 'अन्यत्र', 'अन्यथा', 'अरु', 'अरुलाई', 'अर्को', 'अर्थात', 'अर्थात्', 'अलग', 'आए', 'आजको', 'ओठ', 'आत्म', 'आफू', 'आफूलाई', 'आफ्नै', 'आफ्नो', 'आयो', 'उदाहरण', 'उनको', 'उहालाई', 'एउटै', 'एक', 'एकदम', 'कतै', 'कम से कम', 'कसै', 'कसैले', 'कहाँबाट', 'कहिलेकाहीं', 'का', 'किन', 'किनभने', 'कुनै', 'कुरा', 'कृपया', 'केही', 'कोही', 'गए', 'गरौं', 'गर्छ', 'गर्छु', 'गर्नुपर्छ', 'गयौ', 'गैर', 'चार', 'चाहनुहुन्छ', 'चाहन्छु', 'चाहिए', 'छू', 'जताततै', 'जब', 'जबकि', 'जसको', 'जसबाट', 'जसमा', 'जसलाई', 'जसले', 'जस्तै', 'जस्तो', 'जस्तोसुकै', 'जहाँ', 'जान', 'जाहिर', 'जे', 'जो', 'ठीक', 'तत्काल', 'तदनुसार', 'तपाईको', 'तपाई', 'पर्याप्त', 'पहिले', 'पहिलो', 'पहिल्यै', 'पाँच', 'पाँचौं', 'तल', 'तापनी', 'तिनी', 'तिनीहरू', 'तिनीहरुको', 'तिनिहरुलाई', 'तिमी', 'तिर', 'तीन', 'तुरुन्तै', 'तेस्रो', 'तेस्कारण', 'पूर्व', 'प्रति', 'प्रतेक', 'प्लस', 'फेरी', 'बने', 'त्सपछि', 'त्सैले', 'त्यहाँ', 'थिएन', 'दिनुभएको', 'दिनुहुन्छ', 'दुई', 'देखि', 'बरु', 'बारे', 'बाहिर', 'देखिन्छ', 'देखियो', 'देखे', 'देखेको', 'देखेर', 'दोस्रो', 'धेरै', 'नजिकै', 'नत्र', 'नयाँ', 'निम्ति', 'बाहेक', 'बीच', 'बीचमा', 'भन', 'निम्न', 'निम्नानुसार', 'निर्दिष्ट', 'नौ', 'पक्का', 'पक्कै', 'पछि', 'पछिल्लो', 'पटक', 'पर्छ', 'पर्थ्यो', 'भन्छन्', 'भन्', 'भन्छु', 'भन्दा', 'भन्नुभयो', 'भर', 'भित्र', 'भित्री', 'म', 'मलाई', 'मात्र', 'माथि', 'मुख्य', 'मेरो', 'यति', 'यथोचित', 'यदि', 'यद्यपि', 'यसको', 'यसपछि', 'यसबाहेक', 'यसरी', 'यसो', 'यस्तो', 'यहाँ', 'यहाँसम्म', 'या', 'रही', 'राखे', 'राख्छ', 'राम्रो', 'रूप', 'लगभग', 'वरीपरी', 'वास्तवमा', 'बिरुद्ध', 'बिशेष', 'सायद', 'शायद', 'संग', 'संगै', 'सक्छ', 'सट्टा', 'सधै', 'सबै', 'सबैलाई', 'समय', 'सम्भव', 'सम्म', 'सही', 'साँच्चै', 'सात', 'साथ', 'साथै', 'सारा', 'सोही', 'स्पष्ट', 'हरे', 'हरेक']\n"
    }
   ],
   "source": [
    "print (stopwords.words('nepali'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "{'herself', 'or', 'you', 'couldn', 'because', 'then', 'myself', 'on', 'she', 'during', 'a', 'such', 'needn', 'shan', 'but', 'these', 'who', 'will', 'yourselves', 'your', 'while', \"hasn't\", 'an', \"weren't\", 'not', 'does', 'some', 'what', 'be', 'until', 'doing', 's', 'both', 'own', \"isn't\", 'wouldn', 'after', \"couldn't\", 'very', 'himself', 'were', 't', 'itself', 'too', \"don't\", 'have', 'don', 'and', \"shan't\", 'there', 'm', 'now', 'can', \"hadn't\", 'hasn', 'ma', \"didn't\", 'isn', 'below', 'had', 'll', \"doesn't\", 'haven', 'his', 'by', 'between', 'hadn', \"mightn't\", 'with', 'in', 'y', 'whom', 'my', 'once', \"haven't\", 'themselves', 'before', 'doesn', \"that'll\", 'further', 'was', 'yourself', 'did', 'having', 'about', 'him', 'over', 'shouldn', \"you've\", 'same', 'off', 'where', 'so', 'those', 'their', \"wouldn't\", 'any', \"shouldn't\", 'them', 'theirs', \"she's\", \"aren't\", 'when', 'i', 'why', 'our', 'above', 'is', 'from', 'won', 'at', 'ours', 'are', \"you're\", 'ain', 'for', 'mustn', 'each', 'into', 'most', \"won't\", 'as', 'd', 'down', 're', 'which', \"should've\", 'if', 'wasn', 'yours', 'aren', 'been', 'mightn', 'me', 'o', \"wasn't\", 'this', 'do', 'more', 'through', 'only', 'than', \"you'll\", 'under', 'it', 'being', 'they', 'just', 'that', 'few', 'her', 'against', 'how', 'hers', 'to', 'all', \"needn't\", 'no', 'weren', 'its', 'we', 'has', 'the', 'here', 'again', 'he', \"mustn't\", 'other', 'up', 'nor', 'ourselves', \"it's\", 'should', 'didn', 'of', 'out', 've', \"you'd\", 'am'} \n\nlen(stopWords):  179\n"
    }
   ],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "print(stopWords, '\\n')\n",
    "print('len(stopWords): ', len(stopWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsFiltered = []\n",
    "for w in word_tokens:\n",
    "    if w not in stopWords:\n",
    "        wordsFiltered.append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function to compute what fraction of words in a text are not in the stopwords list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_fraction(word_lst):\n",
    "    stop_words = stopwords.words('english')\n",
    "    content = [w for w in word_lst if w.lower() not in stop_words]\n",
    "    return len(content) / len(word_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', ...] \n 1720901\n"
    }
   ],
   "source": [
    "print(reuters.words(), '\\n', len(reuters.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.735240435097661"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_fraction(reuters.words())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bit4ff258216e894a4b867aae2fac70db47"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}